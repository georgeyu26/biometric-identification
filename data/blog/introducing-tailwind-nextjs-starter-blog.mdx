---
title: 'Facial Recognition API Findings and Results'
date: '2023-04-07'
lastmod: '2023-04-07'
tags: ['face', 'recognition']
draft: false
summary: 'Facial Recognition API Findings and Results'
---

<img src="https://images.ctfassets.net/3viuren4us1n/5YzA7KGIWQEjt8KStZGlxd/85bde9966a9e9c4407396f424e46fc67/facial_recognition.jpg" />

# Facial Recognition API Findings and Results

## Discrimination in Facial Recognition Technology

In today’s age, billions of users are using facial recognition for authentication to unlock their phones and make digital payments. The technology is also used for serious matters such as law enforcement, airport screening, as well as employment and housing decisions. Despite the popularity of facial recognition, it is actually one of the least accurate biometrics, raises privacy concerns, and was even banned for use by police in several cities.

Police use facial recognition technology in order to compare photos of suspects to mugshots and driver’s license portraits; millions of adults have photos contained within facial recognition networks used by law enforcement, even without consent or awareness. Shockingly, these facial recognition technologies present significant racial biases, especially against black skinned people.

Facial recognition algorithms are highly accurate at detecting faces, without 90% classification accuracy. However, these successful results are not universal. Growing amounts of research has suggested that there are inconsistencies across demographic groups. Subjects who are female, black, and aged 18-30 years old consistently have had the poorest accuracy.

In an experiment conducted by the Gender Shades project in 2018, three classification algorithms, including those developed by IBM and Microsoft, were assessed with various subjects. These subjects were grouped into four categories: dark-skinned females, dark-skinned males, light-skinned females, as well as light-skinned males. It was found that all three algorithms had the worst performance on dark-skinned females, with error rates up to 34% higher than light-skinned males.

<img src="https://i0.wp.com/sitn.hms.harvard.edu/wp-content/uploads/2020/10/Najibi_Fig-1-1.png?w=1440&ssl=1" />

<img src="https://www.hibernian-recruitment.com/wp-content/uploads/2022/07/grafik-1.png" />

The results of this experiment prompted an immediate response among the public, bringing into light the issues surrounding biases in facial recognition. Both IBM and Microsoft announced that they would take action to reduce bias in their facial recognition algorithms by modifying testing and improving data collection on specific demographics.

Another important source of racial discrimination with facial recognition lies in its utilization in law enforcement. Many advocates of equality fear that these algorithms could disproportionately harm the black community, as there have been high amounts of racist patterns within law enforcement. Additionally, it can target other marginalized populations, such as immigrants and Muslim citizens.

Discriminatory law enforcement practices were especially visible after the murder of George Floyd by the Minneapolis police department in 2020. Black citizens are statistically more likely to be arrested and incarcerated for minor crimes than white citizens. As a result, black citizens are overrepresented in mugshot photos, which is used by facial recognition algorithms to make predictions.

The unjust use of facial recognition and surveillance by law enforcement threatens citizens, along with their rights, privacy, and freedom of expression. High amounts of surveillance can cause behavioral changes, such as self-censorship, and avoiding activism as a result of fear of retribution. For example, many black lives matter protesters are concerned that facial recognition might monitor and identify them. Surveillance can also induce fear and psychological harm, making subjects highly vulnerable to targeted abuses and physical harm. In criminal justice settings, biases in facial recognition, in severe cases, can even lead to misidentifying and incarcerating innocent black subjects.

## Facial Recognition and Security

Over the past few years, facial recognition technology has made continuous and noteworthy improvements. According to the National Institute of Standards and Technology, the rate of improvement is not slowing down, and it is critical that users of facial recognition technology remain flexible when it comes to implementation; users do not want to be using outdated and potentially insecure biometric algorithms. There are a few attack vectors in particular that are of note, and understanding them can help one understand the security of facial recognition technology.

One of the attack vectors is ‘face swapping’. This technique involves using pre-existing photographs and images of the target, and individually extracting facial features from each image to make a novel collage-like image that can bypass a facial recognition system. Because the resulting image is new and original, it can bypass many security measures of a facial recognition system. There exists free software that can accomplish this in a matter of seconds, given the appropriate input images.

<img src="https://scontent.fykz1-2.fna.fbcdn.net/v/t39.30808-6/299736679_394513486152827_7995016928150933472_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=e3f864&_nc_ohc=Izn8QjsC-0oAX_iYPsu&_nc_ht=scontent.fykz1-2.fna&oh=00_AfB3azVZ2IFZH4PS_gWWbIQVg16V-zOZqe0GW5kQls-rVw&oe=643614D0" />

Over time, researchers have found a countermeasure for this attack by introducing machine learning techniques to detect face swapped images. One paper reported a detection accuracy of over 92%. Because the deep learning space is constantly evolving, and because there are often no shortages of high quality source images, this may remain a permanent risk for facial recognition technology.

## Facial Recognition Findings

We tested faces that fall under the four categories that the Google Cloud Vision API can classify: joy, anger, sorrow, and surprise.

For joy, we found that the Vision API was able to correctly and unambiguously classify the happy faces. This was the case regardless of skin color, ethnicity, and gender.

For anger, the results were less impressive. On the whole, it tended to underestimate how angry a face is, sometimes even classifying an angry face as “Anger: Very Unlikely”. The output was inconsistent, and no clear trend or bias was found for different skin colors, ethnicities, or genders in our testing. The API was able to detect anger more accurately if the face featured a yelling/open mouth and the display of teeth, but for more subtle cues of anger (for example, cues in the eyes, brows, and lips), it was inconsistent and inaccurate.

For sorrow, we noticed a possible trend of inaccuracy in persons of color/minorities. In fact, in an otherwise identically shot photo of a woman expressing clear sorrow (even featuring tears), and the only variable being the woman’s ethnicity, the API gave “Sorrow: Unlikely” for the person of color, and “Sorrow: Very Likely” for the Caucasian woman. The trend continued for men, with “Sorrow: Possible” for an Asian male, and “Sorrow: Likely” for a Caucasian male. This shows the possibility of underestimation of sorrow in minorities/persons of color with the Vision API.

<img src="https://st2.depositphotos.com/7036298/10694/i/950/depositphotos_106940944-stock-photo-brunette-short-hair-adult-caucasian.jpg" />

<img src="https://thelinknewspaper.ca/images/articles/Volume_39/Special/_resized/39.2.SP.ChelseaCameron.ElisaBarbier-11.jpg" />

Lastly, for surprise, it was able to accurately classify all of the surprised faces. However, for the male faces, it also labeled them as “Possibly” and “Likely” angry in addition to being surprised, which was not the case for the female faces. It is unclear if this is a bias or limitation of the API, or if men tend to display a surprised emotion more ambiguously than women.

## References

Najibi, A. (2020, October 26). Racial discrimination in face recognition technology. Science in the News. Retrieved March 20, 2023, from https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/

Gender Shades. (n.d.). Retrieved March 20, 2023, from http://gendershades.org/overview.html
